

압축풀기
# unzip oracle*

alien libaio1 unixodbc를 다운로드 한다.
# apt-get -y install alien libaio1 unixodbc

rpm 파일을 deb파일로 변환한다.
# alien --scripts -d oracle*


deb으로 변환된 프로그램을 설치한다. (시간이 오래걸린다)
# dpkg --install oracle*.deb


wget https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz

하둡 압축 해지
tar -xzvf hadoop-3.3.5.tar.gz

sudo mv hadoop-3.3.5 /usr/local/hadoop


vi /usr/local/hadoop/hadoop-3.3.5/etc/haddop/hadoop-env.sh
출처: https://nirsa.tistory.com/entry/Hadoop-hadoop-error-javahome-is-not-set-and-could-not-be-found [Nirsa:티스토리]




RPM 명령어 사용
rpm 명령어 사용 방법을 간단히 설명하자면 다음과 같습니다.
rpm [옵션] [rpm 패키지파일 또는 rpm 패키지이름]


먼저 rpm 패키지가 이미 설치 되어 있는 상태 인지 조회합니다.
파이프와 grep을 통해 더욱 세밀하게 조회할 수 있습니다.
rpm -qa
rpm -qa 패키지이름
rpm -qa | grep 패키지이름


패키지를 실질적으로 설치하기 위해서는 다음의 옵션을 가장 많이 사용합니다.
rpm -Uvh 패키지파일
U 옵션은 패키지를 설치하되 만약 패키지가 이미 설치된 경우 업그레이드로 진행하라는 뜻입니다.
v 옵션은 패키지 설치시 설치 과정을 출력하라는 뜻입니다.
h 옵션은 설치 진행률을 # 기호로 채워나가도록 화면에 출력하라는 뜻입니다.


패키지를 삭제하는 경우는 e 옵션(erase)을 사용합니다.

rpm -e 패키지이름
출처: https://dololak.tistory.com/312 [코끼리를 냉장고에 넣는 방법:티스토리]



설치    
-i      rpm -i [옵션] 패키지명

업그레이드
-U      rpm -U [옵션] 패키지명
-F      rpm -F [옵션] 패키지명


질의
-q      rpm -q [옵션] 패키지명

검증
-v      rpm -v [옵션] 패키지명

서명 확인
-checksig       rpm --checksig 패키지명

삭제
-e      rpm -e [옵션] 패키지명

데이터베이스 다시 제작
--rebuilddb     rpm --rebuilddb



$ sudo apt update
$ sudo apt install curl
$ sudo yum install curl





에어플로우
#export PATH=$PATH:/home/cjm1s/.local/bin
설치
export AIRFLOW_HOME=~/airflow
export AIRFLOW_VERSION="2.8.0"
export PYTHON_VERSION="$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
export CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
export PATH=$PATH:/home/cjm1s/.local/bin

pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"


zsh
export AIRFLOW_HOME=~/airflow
export PATH=$PATH:/home/cjm1s/.local/bin

방금 데이터베이스를 삭제했습니다.
> rm ~/airflow/airflow.db


airflow users create -u cjm1s -p qkzm0515 -f Clueless -l Coder -r Admin -e cjm1s@naver.com

airflow db init
airflow webserver --port 8080
airflow scheduler

airflow tasks list {dag id}

airflow tasks list {dag id} --tree

airflow dags list

airflow dags test {dag_id} {date}

나: 공장장 / 대그:파이프라인 / 오퍼레이터:기계






curl "https://openapi.naver.com/v1/search/local.xml?query=%EC%A3%BC%EC%8B%9D&display=10&start=1&sort=random" \
    -H "X-Naver-Client-Id: cjQ5E8NZQbmvDcumtAaj" \
    -H "X-Naver-Client-Secret: z87lxXTTsq" -v


가상환경 설치
sudo apt-get update
sudo apt-get install python3-venv python3-pip

가상환경만들기
python3 -m venv test2

source (가상환경 세부 프로젝트)/bin/activate
source ./test2/bin/activate





기본 DAG 작성법 순서
module 추가
Defulat arguments 추가
DAG 작성(id, args, schedul_interval 등)
Task 정의(필요에 따라 doc_md 작성 및 추가)
Dependencies 연결

Workflow
작업 흐름이라는 뜻을 가지고 있고,
가장 작은 단위는 Operator들이고 이것이 모여 Task, task들이 모여 DAG, DAG들이 모여 Workflow가 된다
순서 Workflow << DAGs << Tasks(Operators) 로 커진다고 보면 된다.



Task
하나의 작업 단위를 Task라고 하며 하나 또는 여러 개의 Task를 이용해 하나의 DAG를 생성하게 된다. Task간 >>(downStream), <<(upstream)을 이용하여 어떤 작업을 할지 정할 수 있다.
Operator란?
DAGs가 작동하는 동안 workflow를 어떻게 작동하는지 묘사하는 것이다. 하나의 작업(TASK)에 대해 정의 할 수 있다. 이런 TASK들이 모여서 하나의 DAG를 구성하고 DAG들이 모여서 workflow가 만들어진다. Operator는 DAG를 구성하기 윈한 가장 작은 단위로 봐도 된다.



@once
한 번 실행
@hourly
한 시간에 한 번, 한 시각이 시작할 때 실행
@daily
하루에 한번, 자정에 실행
@weekly
일주일에 한 번, 일요일 자정에 실행
@monthly
한 달에 한 번, 그 달의 첫 날 자정에 실행
@yearly
1년에 한번씩 1월 1일 자정에 실행
출처: https://gngsn.tistory.com/263 [ENFJ.dev:티스토리]




Airflow 디펜던시 설치
sudo apt-get install -y vim ssh openssh-server software-properties-common apt-transport-https wget
sudo apt-get install -y python3
sudo apt-get install -y freetds-bin
sudo apt-get install -y krb5-user
sudo apt-get install -y ldap-utils
sudo apt-get install -y libsasl2-2
sudo apt-get install -y libsasl2-modules
sudo apt-get install -y libssl-dev
sudo apt-get install -y locales
sudo apt-get install -y lsb-release
sudo apt-get install -y sqlite3
sudo apt-get install -y sasl2-bin
sudo apt-get install -y unixodbc 
sudo apt-get install -y python3-pip
sudo apt-get install -y python3-testresources
sudo apt-get install -y mysql-client
sudo apt-get install -y libmysqlclient-dev
sudo apt-get install -y python3-mysqldb



AIRFLOW_VERSION=2.8.0
PYTHON_VERSION="$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip3 install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"



ifconfig 설치
CentOS에 ifconfig 명령어를 설치하려면 yum 명령어를 사용해서 net-tools 패키지를 설치합니다.

# sudo yum install net-tools
Ubuntu에 ifconfig 명령어를 설치하려면 apt-get 명령어를 사용해서 net-tools 패키지를 설치합니다.

# sudo apt-get install net-tools

find보다 빠른 검색 locate명령어(디비를 미리생성해서 빠름)
# sudo apt-get -y install mlocate
    sudo updatedb
    locate 파일이름


삼바 설치
sudo apt-get -y install samba # 우분투 삼바 설치
삼바 계정 등록
sudo smbpasswd -a cjm1s
$ sudo pdbedit -L # 삼바 서버에 등록된 유저를 확인
$ sudo pdbedit -L -v





라이브러리 실행 및 접속
콘솔이나 터미널을 통해 다운로드 받은 DynamoDB 디렉토리로 이동합니다. 그리고 다음의 명령어를 실행해 DynamoDB를 Local에서 실행하시기 바랍니다.(단, 서론에서 말했듯이 콘솔에서 AWS 계정에 대한 configuration 작업이 선행되어 있어야 합니다.)
출처: https://sharplee7.tistory.com/154 [Modern Architecture Stories:티스토리]
java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb


다음의 명령어를 통해 현재 실행된 로컬의 DynamoDB에 접속해 테이블 리스트를 가져와 보도록 합니다.
aws dynamodb list-tables --endpoint-url http://localhost:8000
출처: https://sharplee7.tistory.com/154 [Modern Architecture Stories:티스토리]


테이블 생성
다음의 명령어를 통해 'MusicCollection'이라는 테이블을 생성해 봅니다. 이때 접속 엔드포인트를 localhost로 생성해 로컬의 DynamoDB에 테이블을 생성하도록 합니다. 엔드포인트를 로컬로 지정하지 않으면 aws configuration을 통해 설정한 계정의 DynamoDB에 테이블을 생성합니다.
aws dynamodb create-table \
    --table-name MusicCollection \
    --attribute-definitions AttributeName=Artist,AttributeType=S AttributeName=SongTitle,AttributeType=S \
    --key-schema AttributeName=Artist,KeyType=HASH AttributeName=SongTitle,KeyType=RANGE \
    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 --endpoint-url=http://localhost:8000
출처: https://sharplee7.tistory.com/154 [Modern Architecture Stories:티스토리]


이제 생성된 테이블을 확인해 보도록 합니다.
aws dynamodb list-tables --endpoint-url http://localhost:8000
출처: https://sharplee7.tistory.com/154 [Modern Architecture Stories:티스토리]



데이터 입력
다음의 명령어로 데이터를 입력해 봅니다.
aws dynamodb put-item \
    --table-name MusicCollection \
    --item '{
        "Artist": {"S": "No One You Know"},
        "SongTitle": {"S": "Call Me Today"} ,
        "AlbumTitle": {"S": "Somewhat Famous"} 
      }' \
    --return-consumed-capacity TOTAL --endpoint-url=http://localhost:8000
출처: https://sharplee7.tistory.com/154 [Modern Architecture Stories:티스토리]


데이터 조회
지금 입력한 데이터를 아래의 쿼리를 통해 조회해 보도록 합니다.
aws dynamodb query \
    --table-name MusicCollection \
    --key-condition-expression "Artist = :name" \
    --expression-attribute-values  '{":name":{"S":"No One You Know"}}' --endpoint-url=http://localhost:8000
출처: https://sharplee7.tistory.com/154 [Modern Architecture Stories:티스토리]





